{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Quantitative Distributions\n",
    "    - binning and percentiles\n",
    "* Categorical Distributions\n",
    "    - grouping as smoothing \n",
    "    - additive smoothing\n",
    "* Time Series\n",
    "* Rolling Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separating signal from noise\n",
    "* Information Extraction: select what's important\n",
    "* *Real trends* instead of *coincidence*\n",
    "<img src=\"imgs/signal.jpeg\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separating signal from noise\n",
    "\n",
    "* Filter out 'random fluctuations'\n",
    "* Find the 'right view' of the data\n",
    "* Make trends clearer through 'simplication'\n",
    "\n",
    "Noise to someone else may be signal to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separating signal from noise\n",
    "\n",
    "Related to:\n",
    "* Signal-processing and filters (engineering)\n",
    "* Feature engineering (machine learning)\n",
    "* Modeling (statistics, social sciences)\n",
    "    \n",
    "All of these are intertwined!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothing\n",
    "\n",
    "Smoothing extracts trends from data by reducing the variance of nearby observations.\n",
    "\n",
    "* Origins in engineering (audio/images)\n",
    "* Good for visualization\n",
    "* Careful building inferential models on smoothed data!\n",
    "    - inferential models assess what is/isn't noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions\n",
    "\n",
    "* A **distribution** of a population describes the likelihood that a given value will occur.\n",
    "* The **empirical distribution** of a dataset describes the frequency that each value was observed.\n",
    "    - For a large representative sample, these are 'similar'.\n",
    "* Distributions are defined for both *quantitative* and *categorical* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantitative Distributions\n",
    "\n",
    "* Answers the question: \"where is most the data?\"\n",
    "* A (theoretical) distribution is likelihood a given value occurs at infinite precision.\n",
    "* Samples are approximations:\n",
    "    - How finely you measure proximity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data\n",
    "v = np.concatenate([np.random.normal(x,10,1000//x) for x in range(1,100,10)])\n",
    "np.random.shuffle(v)\n",
    "sensor_data = pd.Series(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '%d instances of observed data' % sensor_data.shape[0]\n",
    "sensor_data.plot(title=title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.sort_values().reset_index(drop=True).plot(title='values of observations, sorted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Empirical Distributions\n",
    "\n",
    "* Empirical distributions are just histograms!\n",
    "* Proximity is measured by bin width\n",
    "* Samples are approximations: how to pick bin width?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"small bin width\": every value happens only once!\n",
    "sensor_data.value_counts().plot(title='frequency of occurence of each value', figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,2, figsize=(12,8))\n",
    "cnt = 0\n",
    "for b in [500, 200, 100, 50, 20, 10, 5, 2]:\n",
    "    sensor_data.plot(kind='hist', bins=b, ax=axes[cnt//2, cnt%2], title='bins=%d' %b, density=True)\n",
    "    cnt += 1\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothing: binning data\n",
    "* Binning quantitative data approximates a probability distribution.\n",
    "* An appropriate choice of bin size reduces noise.\n",
    "    - bin sizes may not be uniform\n",
    "* Decreases precision: assign a value to it's bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdpd example with subject_age\n",
    "stops = pd.read_csv('stops_2016.csv')\n",
    "\n",
    "def clean_age(x):\n",
    "    if pd.notnull(x) and x.isdigit() and int(x) <= 100:\n",
    "        return int(x)\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "ages = stops['subject_age'].apply(clean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one bin per year\n",
    "# spiky every five years! (spikes are noise)\n",
    "# our association of age <-> stops shouldn't vary too much so quickly\n",
    "\n",
    "ages.plot(kind='hist', bins=100, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 bin = 5 years\n",
    "ages.plot(kind='hist', bins=20, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers and bucketing into percentiles\n",
    "* Binning based on relative order limit the effect of outliers.\n",
    "* Label observations by what percentile they're in:\n",
    "    - reduce the number of bins\n",
    "    - outliers no long have their own bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal histogram; outlier ages included\n",
    "all_ages = pd.to_numeric(stops.subject_age, errors='coerce')\n",
    "all_ages.plot(kind='hist', bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram with percentile bins\n",
    "bins = np.percentile(all_ages.dropna().values, range(10, 101, 10))\n",
    "\n",
    "plt.hist(all_ages.dropna(), bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical distributions\n",
    "\n",
    "* **categorical distribution** is the likelihood of a any categorical value occuring.\n",
    "* The empirical distibution is the count/proportion of every category.\n",
    "* For a pandas Series: `Series.value_counts(normalize=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='categorical distribution of subject_race'\n",
    "stops.subject_sex.value_counts(dropna=False, normalize=True).to_frame().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothing categorical distributions\n",
    "* What if there are too many categories?\n",
    "* Approach #1: map categories to coarser categories (using domain knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='categorical distribution of subject_race'\n",
    "stops.subject_race.value_counts(normalize=True).to_frame().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_dict = {'A':'Asian',\n",
    "             'B':'Black',\n",
    "             'C':'Asian',\n",
    "             'D':'Asian',\n",
    "             'F':'Asian',\n",
    "             'G':'Asian',\n",
    "             'H':'Hispanic',\n",
    "             'I':'American Indian',\n",
    "             'J':'Asian',\n",
    "             'K':'Asian',\n",
    "             'L':'Asian',\n",
    "             'O':'OTHER',\n",
    "             'P':'Asian',\n",
    "             'S':'Asian',\n",
    "             'U':'Hawaiian',\n",
    "             'V':'Asian',\n",
    "             'W':'White',\n",
    "             'Z':'Asian'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    stops\n",
    "    .subject_race\n",
    "    .apply(lambda x:race_dict.get(x, np.NaN))\n",
    "    .value_counts(normalize=True, dropna=False)\n",
    "    .to_frame()\n",
    "    .plot(kind='barh')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothing categorical distributions\n",
    "\n",
    "* Problem: no ability to \"coarsen\" using domain knowledge.\n",
    "    - Give up on \"coarsening values\"; replace value with likelihood\n",
    "* Uncommon values are more subject to error from noise.\n",
    "    - rare occurrences should be trusted less!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical Distributions with many distinct values\n",
    "\n",
    "* Conisder a \"representative\" sample of the english language.\n",
    "* Some words occur a lot; most words occur only once.\n",
    "* What happens if our sample is is off by one for:\n",
    "    - the word 'the'?\n",
    "    - the word 'courteous'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = open('pride_and_predjudice.txt').read()\n",
    "text = re.sub('[^0-9a-zA-Z\\s\\n]+', '', text)\n",
    "    \n",
    "words = pd.Series(text.split()).str.lower().loc[lambda x:x.apply(len) <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words\n",
    "word_cnts = words.value_counts()\n",
    "word_cnts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And these are only 3% of the words!\n",
    "words.value_counts().iloc[:200].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical distribution of \"the\"\n",
    "(word_cnts.loc['the'] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical distribution of \"the\" (off by a count of one 1)\n",
    "((word_cnts.loc['the'] + 1) / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical distribution of \"courtious\"\n",
    "(word_cnts.loc['courteous'] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical distribution of \"the\" (off by a count of one 1)\n",
    "((word_cnts.loc['courteous'] + 1) / len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additive Smoothing\n",
    "* If we see a value a lot:\n",
    "    - the likelihood of occurance ~ similar to what's observed.\n",
    "* If we've rarely seen a category:\n",
    "    - Is it really that rare? \n",
    "\n",
    "Noise causes more error in the tail of the distribution!\n",
    "\n",
    "see: [ref](https://en.wikipedia.org/wiki/Additive_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additive Smoothing\n",
    "* Given a dataset of observations $x$ of size $N$, with $d$ categories,\n",
    "* Smooth the empirical probability a value occurs:\n",
    "$$ p_i = \\frac{x_i}{N} \\qquad {\\rm (empirical)}$$\n",
    "\n",
    "$$ p_i = \\frac{x_i + \\alpha}{N + \\alpha d} \\qquad {\\rm (smoothed)}$$\n",
    "\n",
    "* Where $\\alpha$ reflects a guess that each category has an additional count $\\alpha$\n",
    "* Where $1/d$ is the uniform probability, if each category is equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additive Smoothing\n",
    "$$ p_i = \\frac{x_i + \\alpha}{N + \\alpha d} \\qquad {\\rm (smoothed)}$$\n",
    "\n",
    "* If $\\alpha$ is zero, then \n",
    "\n",
    "$$ p_i = \\frac{x_i + \\alpha}{N + \\alpha d} = \\frac{x_i + 0}{N + 0\\cdot d} = \\frac{x_i}{N} \\qquad {\\rm (empirical\\ prob)}$$\n",
    "\n",
    "* If $\\alpha >> 0$, then\n",
    "\n",
    "$$ p_i = \\frac{x_i + \\alpha}{N + \\alpha d} \\approx \\frac{\\alpha}{\\alpha\\cdot d} = \\frac{1}{d} \\qquad {\\rm (uniform\\ prob)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additive Smoothing: SDPD race codes\n",
    "* $N$ -- number of traffic stops\n",
    "* $d$ -- distinct values of race code\n",
    "* $\\alpha$ -- smoothing parameter\n",
    "* $x_i$ -- number of stops for a given race code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(stops)\n",
    "d = stops.subject_race.nunique()\n",
    "alpha = 100 # effect: over count by 1000 stops!\n",
    "\n",
    "cnts = stops.subject_race.value_counts(dropna=False)\n",
    "emp = stops.subject_race.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = ((cnts + alpha)/(N + alpha * d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([emp.rename('empirical distribution'), smoothed.rename('smoothed')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additive smoothing: changing $\\alpha$\n",
    "* What happens to the distributions of race codes as $\\alpha\\to\\infty$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter list 0 to 100k\n",
    "alphas = 10**np.array(range(7))\n",
    "alphas = np.append(0, alphas)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate smoothed distributions\n",
    "smooth_list = []\n",
    "for alpha in alphas:\n",
    "    smoothed = ((cnts + alpha)/(N + alpha * d)).rename(alpha)\n",
    "    smooth_list.append(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot smoothed distributions for different values of alpha\n",
    "pd.concat(smooth_list, axis=1).plot(kind='bar', subplots=True, title=False, figsize=(15,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform distribution by number of categories\n",
    "1 / d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bivariate additive smoothing: incidence rate\n",
    "* examine a categorical attribute $x$ using a second, boolean attribution $b$.\n",
    "\n",
    "$$ p_i = \\frac{b_i}{x_i} \\qquad {\\rm (empirical)}$$\n",
    "\n",
    "$$ p_i = \\frac{N\\cdot(b_i/x_i) + \\alpha\\cdot(b/N)}{N + \\alpha} \\qquad {\\rm (smoothed)}$$\n",
    "\n",
    "* Additive smoothing interpolates between:\n",
    "    1. incidence rate per group, and\n",
    "    2. overall incidence rate.\n",
    "* More covered later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Bivariate additive smoothing: vehicle stop search rates\n",
    "\n",
    "* Search rates by race_code are noisy for small groups\n",
    "* Additive smoothing interpolates between:\n",
    "    1. search rate per group, and\n",
    "    2. overall search rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def clean_arrested(s):\n",
    "    if s in ['N', 'n']:\n",
    "        return 0\n",
    "    elif s in ['Y', 'y']:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "stops['arrested'] = stops.arrested.apply(clean_arrested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Arrest Rates additive smoothing\n",
    "\n",
    "N = len(stops)\n",
    "b = stops.arrested.sum()\n",
    "alpha = 100\n",
    "\n",
    "arrests_by_subject_race = stops.groupby('subject_race').arrested.sum()\n",
    "stops_by_subject_race = stops.subject_race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "b/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "smoothed = (N * (arrests_by_subject_race / stops_by_subject_race) + alpha * (b/N))/(N + alpha)\n",
    "smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rolling Windows\n",
    "* Group data in buckets/windows and compute statistics on the buckets\n",
    "* windows should overlap, to account for sudden changes\n",
    "* Very common to analyze sequences of events, like time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our \"sensor data\" with a lot of noise\n",
    "sensor_data.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avering over windows of 50 observations\n",
    "sensor_data.rolling(window=50).mean().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rolling Windows in Pandas\n",
    "* What just happened?\n",
    "    - `rolling` method 'splits' the dataframe into overlapping windows.\n",
    "    - apply the desired method to each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "\n",
    "df = pd.DataFrame( {'numbers': [1,1,1,2,2,2,3,3,3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the moving sum. That is, take\n",
    "# the first two values, sum them, \n",
    "# then drop the first and add the third, etc.\n",
    "# can use any aggregation function\n",
    "\n",
    "df.rolling(window = 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create outliers\n",
    "\n",
    "df = pd.DataFrame( {'numbers': [1,1,1,20,2,2,3,30,3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing with average with different window sizes\n",
    "\n",
    "df[\"Window = 2\"]=df[\"numbers\"].rolling(window = 2).mean()\n",
    "df[\"Window = 3\"]=df[\"numbers\"].rolling(window = 3).mean()\n",
    "df[\"Window = 4\"]=df[\"numbers\"].rolling(window = 4).mean()\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "# which window size is better? \n",
    "# Is there an optimal size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(8,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series\n",
    "\n",
    "* Sequential data is often noisy\n",
    "* Time series data are a common example of sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datetime objects\n",
    "\n",
    "* A **date object** is  a set of values for the *year*, the *month*, the *day*, and a collection of functions that knows how to handle them.\n",
    "\n",
    "* A **time object** is constructed in a similar way. \n",
    "\n",
    "![](./imgs/datetime_attributes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dates and Times in Pandas\n",
    "\n",
    "Often we have a problem with inconsistent structure of input data.\n",
    "\n",
    "How many ways can you come up with to record today's date?\n",
    "\n",
    "* 01-24-2019\n",
    "* 24-01-2019\n",
    "* 01.24.2019\n",
    "* 24.01.2019\n",
    "* 01/24/2019\n",
    "* 24/01/2019\n",
    "* Jan, 24 2019\n",
    "* 24 Jan, 2019\n",
    "*  ....\n",
    "\n",
    "How about the time?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Is it raining in Seattle?\n",
    "\n",
    "```\n",
    "DATE = the date of the observation\n",
    "PRCP = the amount of precipitation, in inches\n",
    "TMAX = the maximum temperature for that day, in degrees Fahrenheit\n",
    "TMIN = the minimum temperature for that day, in degrees Fahrenheit\n",
    "RAIN = TRUE if rain was observed on that day, FALSE if it was not\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"seattleWeather.csv\")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what type is the date column?\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing dates to `datetime` objects\n",
    "\n",
    "* `DATE` column contains date as a string object. \n",
    "* Option 1: use string manipulations to extract needed information by carefully slicing each string. \n",
    "    - if your data is not consistent then this approach fails.\n",
    "* Option 2: convert `object` type to a special `datetime` format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "weather.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice I did not have to specify what number is a month and what number is a day. \n",
    "\n",
    "dates = ['2019-01-22', 'Jan 22, 2019', '01/22/2019', '2019.01.22', '2019/01/22','20190122']\n",
    "pd.to_datetime(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specifying formats for parsing dates/times\n",
    "* `to_datetime` tries guessing the format\n",
    "* Sometimes, formats are ambiguous:\n",
    "    * **US**:  MM/DD/YEAR\n",
    "    * **EUROPE**: DD/MM/YEAR\n",
    "    * **JAPAN**, CHINA: YEAR/MM/DD\n",
    "* use the `format` keyword and the [format reference](http://strftime.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not expect it to always work\n",
    "\n",
    "dates = ['09/01/2019']   # what date is it?\n",
    "pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['09/01/2019']   # what date is it?\n",
    "pd.to_datetime(dates, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datetime objects and the `dt` namespace\n",
    "* many built-in methods to use\n",
    "* see a full list of methods [here](https://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = weather[\"DATE\"].dt.month\n",
    "months.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months.plot(kind='hist', bins=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use time stamps to compare dates/times\n",
    "\n",
    "time_stamp = pd.to_datetime('10/10/1980')\n",
    "weather.loc[weather['DATE'] <= time_stamp].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math operations are also possible on dates:\n",
    "# What day had the most amount of rain?\n",
    "\n",
    "weather.loc[weather['PRCP'] == weather['PRCP'].max()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Deltas\n",
    "\n",
    "* Differences between `datetimes` are called `timedelta` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can subtract dates as well, creating a timedelta object. \n",
    "\n",
    "weather.DATE.max() - weather.DATE.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Data Exploration\n",
    "* Daily precpitation is noisy!\n",
    "* How can we spot trends in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the weather data\n",
    "\n",
    "%matplotlib nbagg\n",
    "%matplotlib\n",
    "\n",
    "weather.plot(y='PRCP', x=\"DATE\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at last two years\n",
    "\n",
    "cut_off = pd.to_datetime('1/1/2016')\n",
    "\n",
    "weather_recent = (\n",
    "    weather\n",
    "    .loc[weather['DATE'] >= cut_off]\n",
    "    .set_index('DATE')\n",
    ")\n",
    "\n",
    "weather_recent.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_recent.plot(y='PRCP');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use rolling window approach to smooth the data\n",
    "\n",
    "smoothed = weather_recent.rolling(window = '30D').mean()\n",
    "smoothed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed.plot(y='PRCP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different smoothing windows\n",
    "# \n",
    "windows =  ['1D', '7D', '30D', '60D', '90D', '180D']\n",
    "smoothed_list = [weather_recent.rolling(window=win).PRCP.mean().rename(win) for win in windows]\n",
    "\n",
    "pd.concat(smoothed_list, axis=1).plot(subplots=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.set_index('DATE').PRCP.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is precipitation changing year-after-year?\n",
    "\n",
    "weather.set_index('DATE').rolling(window='30D').PRCP.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-year rolling windows take away the seasonality: good years and bad years\n",
    "weather.set_index('DATE').rolling(window='365D').PRCP.mean().loc['1950':].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we see longer droughts?\n",
    "weather.set_index('DATE').rolling(window='3650D').PRCP.mean().loc['1955':].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does it look like taking 10 year increments?\n",
    "weather.set_index('DATE').loc['1950':'2020'].groupby(pd.Grouper(freq='3650D')).PRCP.mean().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between rolling windows and aggregation\n",
    "#\n",
    "# aggregation\n",
    "weather.set_index('DATE').loc['1950':'2020'].groupby(pd.Grouper(freq='365D')).PRCP.mean().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling window\n",
    "#\n",
    "weather.set_index('DATE').rolling(window='365D').PRCP.mean().loc['1950':].plot();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
