{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 03\n",
    "\n",
    "### Due Date: Monday, Jan 28 12:00PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw03.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw03.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw03 as hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The other side of the Gradescope\n",
    "\n",
    "In this question you will help me to clean a dataset `grades.csv` that I downloaded from the Gradescope. I removed `Name`, `ID` and `Email` and a few irrelevant columns but the rest of the data was left unchanged. Each subproblem will ask you to perform different manipulations on this table. When you write functions, do not forget about the `DRY` principle: Don't Repeat Yourself. Use helper methods instead of copy/pasting. Think about efficiency as well, not just correctness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful reminder**\n",
    "\n",
    "You can write a function that take the `unknown` number of arguments. Similar to Java's `main (String[] args)`, where you could pass any number of arguments on the command line. \n",
    "\n",
    "For example, you want to write a function that multiple given numbers but you do not know how many numbers will be given. In this case you can use a `*` before the name of the formal parameter:\n",
    "\n",
    "```\n",
    "def mult_them(*nums):\n",
    "    product = 1\n",
    "    for n in nums:\n",
    "        product *=n\n",
    "    print(product)\n",
    "\n",
    ">>> mult_them(1,2,3)\n",
    "6\n",
    ">>> mult_them(1,2,3,4)\n",
    "24\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "Write a function `major_drop` that drops the following columns:\n",
    "* Every column about the `Lab`\n",
    "* Every column about the `Quiz`\n",
    "* Every column about the `Project`\n",
    "* Every column that says `Midterm` and `Lateness` in it\n",
    "* Every column that says `Final` and `Lateness` in it\n",
    "* Column with `Total Lateness`\n",
    "* Every exam (midterm and final) column that contains max points. \n",
    "\n",
    "and return a new dataframe without these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    "* If you inspect the table carefully you will see that there are a lot of `NaN`s. Replace them with the appropriate values. \n",
    "\n",
    "* Look at the columns for the `Midterm`. There are three columns that correspond to three different versions. You need to merge these columns into one, create a new column `Midterm Grades` with the new grades and drop the old ones. \n",
    "\n",
    "* Repeat the same steps for columns about the `Final exam`.\n",
    "\n",
    "You should write a function `merged_exams` that takes in a dataframe  and returns the updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradescope: how late, how early?\n",
    "\n",
    "You are provided with ten files names `hwX_time.csv` that include the dates and times for each submission. When the homework was submitted and how late it was submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    "Your next modification will involve the lateness of each homework. The late submission rule was:\n",
    "* if an assignment is submitted within 24 hours after the deadline, then the penalty is 20%\n",
    "* if an assignment is submitted after 24 hours after the deadline, then the penalty is 50%.\n",
    "\n",
    "\n",
    "If you look closer you will see that some times are way larger than the allowed time frames. It indicates that there was a regrade request and the code was resubmitted. It does not count as late submission. \n",
    "\n",
    "* Write a function `read_all(dirName)` that reads all files from the given directory and returns a list of dataframes. \n",
    "\n",
    "* Then write a function `extract_and_create` that takes in a list of dataframes with times for 10 homeworks and creates a new dataframe with two columns: `Penalty_20` and `Penalty_50`. Each item in the column represents the total number of the corresponding late submissions. \n",
    "\n",
    "For example, if a student submitted 15 hours late for two assignments, 37 hours late for five assignments and 120 hours late for one assignment, then `Penalty_20` and `Penalty_50` will contain 2 and 5 correspondingly. Thus, the regrades should not effect the penalties.\n",
    "\n",
    "*Note: In reality we do take care of the case when assignment was late AND resubmitted for a regrade request*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful hints:**\n",
    "1. You may find it useful to go back to lecture 1 and understand the code that reads in multiple files from a directory. `os` module is imported for you. \n",
    "2. You can replace NaN with `00:00:00` time stamp since it does not effect the answer.\n",
    "3. There are two ways you can take to deal with times: either split them by `:` , convert each item to a number and go from there; or you can use `to_timedelta` method to work with times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Survey\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "Three friends decide to send out a survey to 1000 of their linkedin connections, asking them for their favorite animal. Each friend also records some other data on their connection's pages (company, job, firstname, and summary/slogan). Collect all the data contained in `linkedin1.csv, linkedin2.csv, linkedin3.csv` into a single dataframe and compute some initial summary statistics:\n",
    "\n",
    "* Create a function `compute_stats` that takes in 3 file handles like `open('linkedin*.csv)` and returns a list containing, the most common first name, job held, slogan, and favorite animal (in that order). If there are ties for the most common value, give the value with the \"largest size\" (as defined by the python string ordering).\n",
    "\n",
    "\n",
    "*Note:* Your code will be tested on samples of the three dataframes given above. Don't overly generalize your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful function:**\n",
    "Sometime it is useful to combine two lists into one and it can be done fast by using a function `zip`. It takes in iterable containers, aggregates them into  tuples and returns an iterator of these tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "zipped = zip(x, y)\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then depending on the data you have aggregated, you can use `list` or `dict` constructors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salaries Grouping\n",
    "\n",
    "### `groupby.transform`: transforming data by groups.\n",
    "\n",
    "* The `apply` and `aggregate` methods on groupby expect a function that takes a dataframe (corresponding to a group) and returns a number. The output dataframe is indexed by the values of the groupby column(s) and the columns consist of the values of the function passed into the method.\n",
    "* The `transform` method, on the other hand, expects a function that takes a dataframe (corresponding to a group) and returns a transformed dataframe of the same size. The `transform` method then combines these dataframes into a dataframe of the same shape as the original, full dataframe being grouped. This is useful if you'd like to scale columns, where that scaling depends on the group.\n",
    "\n",
    "For example, each element in `df` below is scaled by the range of the group to which it belongs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar'],\n",
    "                   'B' : ['one', 'one', 'two', 'three',\n",
    "                          'two', 'two'],\n",
    "                   'C' : [1, 5, 5, 2, 5, 5],\n",
    "                   'D' : [2.0, 5., 8., 1., 2., 9.]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('A')\n",
    "grouped.transform(lambda x: x/(x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming SD employee salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall the dataset of salaries for city of San Diego employees from [lecture 1](../../lecture/01/Lecture\\ 01\\ Introduction.ipynb). In our investigation of whether women make total salaries similar to the general population of city employees, we came to the following conclusions:\n",
    "* Women's total salary is on average significantly lower than city employees as a whole.\n",
    "* Much of this difference is due to different gender proportions in different job types. For example fire fighters make a lot more than librarians.\n",
    "\n",
    "The natural follow-up question is whether the same difference is present *when we control for job type*. To approach this, we would like to\n",
    "1. Define different job types (`Job Title` is a messy field).\n",
    "2. *Standardize* salaries within each job type.\n",
    "4. Perform a hypothesis tests as in lecture 1, with the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('san-diego-2017.csv')\n",
    "salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:**\n",
    "\n",
    "* Create a function `job_word_distribution` that takes in a series like `salaries['Job Title']` and returns a series of counts of how many times each word occurred in the column. Assume that words are delimited by whitespace. *Note:* do *not* use loops!\n",
    "\n",
    "Look at the distribution of words in `salaries['Job Title']` -- which ones would make reasonable labels for classifying most Job Titles into a few Job Types? Compare to the list given below. When we cover text processing, we will refine our task here.\n",
    "\n",
    "Assume we care about the following job types:\n",
    "```\n",
    "job_types = ['Police', 'Fire', 'Libr', 'Rec', 'Grounds', 'Lifeguard', 'Water', 'Equip', 'Utility', 'Clerical', 'Administrative', 'Sanitation', 'Principal', 'Public', 'Dispatcher']\n",
    "```\n",
    "\n",
    "A job title belongs to a job type above if any of the above strings (in `job_types`) is a substring of a given Job Title. What proportion of Job Titles have a corresponding job type? (Verify for yourself!) If there isn't a matching job type, then set the job type of that job to be `Other`.\n",
    "\n",
    "Now we are ready to analyze salaries by job-type and standardize them. \n",
    "* First, create a function `describe_salaries_by_job_type` that takes in a dataframe like `salaries` and outputs a dataframe of descriptive statistics of `Total Pay` by `Job Type` (using the method `.describe`).\n",
    "* Then create a function `std_salaries_by_job_type` that takes in a dataframe like `salaries` and outputs a dataframe with \n",
    "    - the same rows as the input,\n",
    "    - four columns given by `['Job Type', 'Base Pay', 'Overtime Pay', 'Total Pay']`,\n",
    "    - where each of the `Pay` columns are *standardized by Job type* -- that is, row is put into the standard units for the job type it belongs to. For a review of standard units, see the [DSC 10 Textbook](https://www.inferentialthinking.com/chapters/15/1/Correlation)\n",
    "    - *Hint*: use the `groupby` method `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at the normalized salaries:\n",
    "\n",
    "# pd.plotting.scatter_matrix(hw.std_salaries_by_job_type(salaries));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (OPTIONAL)**\n",
    "\n",
    "Perform the hypothesis test from lecture 1 on these standardized salaries to answer the question of \"do women earn fair pay, when controlling for job type?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Percentiles\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "Since the total pay of city employees has a *long tail* (i.e. a small number of people make much more than the rest), the results from calculating with means tends to be skewed. That is, *most* people make very little compared to a few well payed employees. In this case, if you care about what the \"typical\" employee earns, it makes sense to bin salaries into percentiles and work with those.\n",
    "\n",
    "* Create a function `bucket_total_pay` that takes in a series like `salaries['Total Pay']` and outputs an array containing the decile of `Total Pay` each employee lies in (deciles are labeled 1-10).\n",
    "* Create a function `mean_salary_per_decile` which takes in a dataframe like `salaries` and outputs a series, indexed by decile, of the mean total pay of each decile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting (optional) excercise, is to redo the analysis of salaries from both lecture 1, as well above, using salary deciles instead of mean salaries.\n",
    "\n",
    "This is a very typical workflow for data scientists -- constantly refine your features and statistics, and re-run your analyses with those. For this reason, developing your analyses like software is highly productive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robo-calls and Marketing\n",
    "\n",
    "Given in `phones.csv` is a synthetically generated list of people's names and their phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phones = pd.read_csv('phones.csv')\n",
    "phones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Suppose you have an upstart robo-dialing service advertising [your IRS scam](https://nypost.com/2018/12/14/these-were-the-most-common-types-of-robocalls-in-2018/); the dataframe `phones` contains the information of 1000 people you need your software to call. Your software needs a dataframe as input satisfying the following conditions:\n",
    "\n",
    "1. The columns should be `id`, `first_name`, `last_name`, `phone`, in that order.\n",
    "2. The `phone` should a 10 digit integer of *string type*.\n",
    "3. Additionally, the `phone` column should contain:\n",
    "    - the `cell_phone` number if it exists,\n",
    "    - otherwise the `home_phone` if it exists, \n",
    "    - otherwise `work_phone` if it exists;\n",
    "    - otherwise `NaN`.\n",
    "\n",
    "\n",
    "Create a function `robo_table` that takes in a dataframe like `phones` and outputs the table described above. Your function should not change the `phones` table.\n",
    "\n",
    "*Hint #1:* Read [lecture 3 on `NaN` and Data Types](https://github.com/ucsd-ets/dsc80-wi19/blob/master/lecture/03/Lecture%2003%20Messy%20Data.ipynb).\n",
    "\n",
    "*Hint #2:* `fillna` will be useful in creating the phone column\n",
    "\n",
    "*Note:* robo-dialing is both illegal and a public nuisance. However, the situation described in this problem is core to most marketing by legal companies large and small. A company's practices can be a nuisance, while still being legal, and there are companies everywhere on this spectrum. Data scientists have a hand in all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeting an age group\n",
    "\n",
    "**Question 8**\n",
    "\n",
    "Next, you would like to target your calls toward the 40-49 age group. Since you don't have age columns, you need to make a best guess at peoples ages. To do this, use the *names* dataset from the Social Security administration in the `names` directory. \n",
    "\n",
    "1. Create a function `read_names` which takes in a directory path (containing files `yob****.txt`), and outputs a dataframe with four columns (`year,first_name,sex,number`). The column `year` denotes the year-of-birth of the recorded name/sex/cnt row, given in the file name.\n",
    "2. Assign the 'best guess' for the age of a given first name as the year in the *names* dataset which contains the most occurances of that name. Create a function `age_guess` which takes in a dataframe as in part 1, and outputs a pd.Series, indexed by name, of the most likely age of each name.\n",
    "3. Lastly, select the rows of `phones` that are most likely between the ages 40-49 (inclusive). Create a function `get_age_group` which takes in a dataframe like `phones` and a series like the output of `age_guess`, and outputs a dataframe consisting of the rows of `phones` who are most likely between the ages of 40-49.\n",
    "\n",
    "*Hint:* Look at the example in the first lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, your done with the homework\n",
    "\n",
    "### Now, run your doctests and upload `hw03.py` to GradeScope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
