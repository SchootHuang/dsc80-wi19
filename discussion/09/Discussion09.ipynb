{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Regex and Pipelining, Column Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very powerful tool.\n",
    "- Used to identify whether a pattern exists in a given sequence of characters (string) or not.\n",
    "- Helpful in manipulating textual data, very important for text mining.\n",
    "- Examples - Validattion of the format of email addresses or password during registration, used for parsing text data files to find, replace or delete certain string, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match!\n"
     ]
    }
   ],
   "source": [
    "#Ordinary characters\n",
    "pattern = r\"Cookie\"\n",
    "sequence = \"Cookie\"\n",
    "if re.match(pattern, sequence):\n",
    "  print(\"Match!\")\n",
    "else: print(\"Not a match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a **raw string literal**. It changes how the string literal is interpreted. Such literals are stored as they appear.\n",
    "\n",
    "For example, \\ is just a backslash when prefixed with a **r rather than being interpreted as an escape sequence**. You will see what this means with special characters. Sometimes, the syntax involves backslash-escaped characters and to prevent these characters from being interpreted as escape sequences, you use the raw r prefix. You don't actually need it for this example, however it is a good practice to use it for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **re.search()** method takes a regular expression pattern and a string and searches for that pattern within the string. If the search is successful, search() returns a **match object** or **None** otherwise. Therefore, the search is usually immediately followed by an if-statement to test if the search succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#basic format of search function\n",
    "match = re.search(pat, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found word:cat\n"
     ]
    }
   ],
   "source": [
    "str = 'an example word:cat!!'\n",
    "match = re.search(r'word:\\w\\w\\w', str)\n",
    "# If-statement after search() tests if it succeeded\n",
    "if match:\n",
    "  print ('found', match.group()) ## 'found word:cat'\n",
    "else:\n",
    "  print ('did not find')\n",
    "\n",
    "# match.group() is the matching text (e.g. 'word:cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Patterns\n",
    "The power of regular expressions is that they can specify patterns, not just fixed characters. Here are the most basic patterns which match single chars:\n",
    "\n",
    "- **a, X, 9, <** -- ordinary characters just match themselves exactly. The meta-characters which do not match themselves because they have special meanings are: **. ^ $ * + ? { [ ] \\ | ( )** (details below)\n",
    "\n",
    "-  **. (a period)** -- matches any single character except newline '\\n'\n",
    "- **\\w** -- (lowercase w) matches a \"word\" character: a letter or digit or underbar [a-zA-Z0-9_]. **Note that although \"word\" is the mnemonic for this, it only matches a single word char, not a whole word**. \\W (upper case W) matches any non-word character.\n",
    "\n",
    "\n",
    "- **\\s** -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form [ \\n\\r\\t\\f]. \\S (upper case S) matches any non-whitespace character.\n",
    "\n",
    "- **\\t, \\n, \\r** -- tab, newline, return\n",
    "\n",
    "- **\\d** -- decimal digit [0-9] (some older regex utilities do not support but \\d, but they all support \\w and \\s)\n",
    "\n",
    "- **^ = start, $ = end** -- match the start or end of the string\n",
    "\n",
    "- **\\** -- inhibit the \"specialness\" of a character. So, for example, use \\. to match a period or \\\\ to match a slash. If you are unsure if a character has special meaning, such as '@', you can put a slash in front of it, \\@, to make sure it is treated just as a character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re.search vs re.match\n",
    "- The match() function checks for a match only at the beginning of the string (by default) whereas the search() function checks for a match anywhere in the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **search(pattern, string, flags=0)**\n",
    "- With this function, you scan through the given string/sequence looking for the first location where the regular expression produces a match. It returns a corresponding match object if found, else returns None if no position in the string matches the pattern. Note that None is different from finding a zero-length match at some point in the string.\n",
    "- **All of the pattern must be matched, but not all of the string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookie'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"cookie\"\n",
    "sequence = \"Cake and cookie\"\n",
    "\n",
    "re.search(pattern, sequence).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **match(pattern, string, flags=0)**\n",
    "- Returns a corresponding match object if zero or more characters at the beginning of string match the pattern. Else it returns None, if the string does not match the given pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"C\"\n",
    "sequence1 = \"IceCream\"\n",
    "\n",
    "# No match since \"C\" is not at the start of \"IceCream\"\n",
    "re.match(pattern, sequence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence2 = \"Cake\"\n",
    "\n",
    "re.match(pattern,sequence2).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ## Search for pattern 'iii' in string 'piiig'.\n",
    "  ## All of the pattern must match, but it may appear anywhere.\n",
    "  ## On success, match.group() is matched text.\n",
    "  match = re.search(r'iii', 'piiig') # found, match.group() == \"iii\"\n",
    "  match = re.search(r'igs', 'piiig') # not found, match == None\n",
    "\n",
    "  ## . = any char but \\n\n",
    "  match = re.search(r'..g', 'piiig') # found, match.group() == \"iig\"\n",
    "\n",
    "  ## \\d = digit char, \\w = word char\n",
    "  match = re.search(r'\\d\\d\\d', 'p123g') # found, match.group() == \"123\"\n",
    "  match = re.search(r'\\w\\w\\w', '@@abcd!!') # found, match.group() == \"abc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetition\n",
    "You use + and * to specify repetition in the pattern\n",
    "\n",
    "- \"+\" -- 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's\n",
    "- \"*\" -- 0 or more occurrences of the pattern to its left\n",
    "- \"?\" -- match 0 or 1 occurrences of the pattern to its left\n",
    "\n",
    "### Leftmost & Largest\n",
    "First the search finds the leftmost match for the pattern, and second it tries to use up as much of the string as possible -- i.e. + and * go as far as possible (the + and * are said to be **\"greedy\" - match as much as possible**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Repetition Examples\n",
    "\n",
    "  ## i+ = one or more i's, as many as possible.\n",
    "  match = re.search(r'pi+', 'piiig') # found, match.group() == \"piii\"\n",
    "\n",
    "  ## Finds the first/leftmost solution, and within it drives the +\n",
    "  ## as far as possible (aka 'leftmost and largest').\n",
    "  ## In this example, note that it does not get to the second set of i's.\n",
    "  ## important example\n",
    "  match = re.search(r'i+', 'piigiiii') # found, match.group() == \"ii\"\n",
    "\n",
    "  ## \\s* = zero or more whitespace chars\n",
    "  ## Here look for 3 digits, possibly separated by whitespace.\n",
    "  match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx') # found, match.group() == \"1 2   3\"\n",
    "  match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx') # found, match.group() == \"12  3\"\n",
    "  match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx') # found, match.group() == \"123\"\n",
    "\n",
    "  ## ^ = matches the start of string, so this fails:\n",
    "  match = re.search(r'^b\\w+', 'foobar') # not found, match == None\n",
    "  ## but without the ^ it succeeds:\n",
    "  match = re.search(r'b\\w+', 'foobar') # found, match.group() == \"bar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you want to check for exact number of sequence repetition?\n",
    "\n",
    "For example, checking the validity of a phone number in an application. re module handles this very gracefully as well using the following regular expressions:\n",
    "\n",
    "**{x} - Repeat exactly x number of times.**\n",
    "\n",
    "**{x,} - Repeat at least x times or more.**\n",
    "\n",
    "**{x, y} - Repeat at least x times but no more than y times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0987654321'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'\\d{9,10}', '0987654321').group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy vs Non-Greedy Matching\n",
    "When a special character matches as much of the search sequence (string) as possible, it is said to be a \"Greedy Match\". It is the normal behavior of a regular expression but sometimes this behavior is not desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h1>TITLE</h1>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"cookie\"\n",
    "sequence = \"Cake and cookie\"\n",
    "\n",
    "heading  = r'<h1>TITLE</h1>'\n",
    "re.match(r'<.*>', heading).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"The pattern <.*> matched the whole string, right up to the second occurrence of >.\n",
    "\n",
    "However, if you only wanted to match the first <h1> tag, you could have used the greedy qualifier *? that matches as little\n",
    "text as possible.\n",
    "\n",
    "Adding ? after the qualifier makes it perform the match in a non-greedy or minimal \n",
    "fashion; That is, as few characters as possible will be matched. When you run <.*>, you will only get a match with <h1>.\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h1>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading  = r'<h1>TITLE</h1>'\n",
    "re.match(r'<.*?>', heading).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b@google\n"
     ]
    }
   ],
   "source": [
    "#Email Example\n",
    "\n",
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "match = re.search(r'\\w+@\\w+', str)\n",
    "if match:\n",
    "  print (match.group())  ## 'b@google'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search does not get the whole email address in this case because the \\w does not match the '-' or '.' in the address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square Brackets\n",
    "Square brackets can be used to indicate a set of chars, so [abc] matches 'a' or 'b' or 'c'. The codes \\w, \\s etc. work inside square brackets too with the one **exception that dot (.) just means a literal dot**. For the emails problem, the square brackets are an easy way to add '.' and '-' to the set of chars which can appear around the @ with the pattern r'[\\w.-]+@[\\w.-]+' to get the whole email address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice-b@google.com\n"
     ]
    }
   ],
   "source": [
    "match = re.search(r'[\\w.-]+@[\\w.-]+', str)\n",
    "if match:\n",
    "    print (match.group())  ## 'alice-b@google.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use a dash to indicate a range, so [a-z] matches all lowercase letters. **To use a dash without indicating a range, put the dash last, e.g. [abc-].** **An up-hat (^) at the start of a square-bracket set inverts it, so [^ab] means any char except 'a' or'b'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Extraction\n",
    "\n",
    "The \"group\" feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. **To do this, add parenthesis ( ) around the username and host in the pattern**, like this: r'([\\w.-]+)@([\\w.-]+)'. In this case, the parenthesis do not change what the pattern will match, instead they establish logical \"groups\" inside of the match text. On a successful search, match.group(1) is the match text corresponding to the 1st left parenthesis, and match.group(2) is the text corresponding to the 2nd left parenthesis. The plain match.group() is still the whole match text as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice-b@google.com\n",
      "alice-b\n",
      "google.com\n"
     ]
    }
   ],
   "source": [
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "match = re.search(r'([\\w.-]+)@([\\w.-]+)', str)\n",
    "if match:\n",
    "    print (match.group())   ## 'alice-b@google.com' (the whole match)\n",
    "    print (match.group(1))  ## 'alice-b' (the username, group 1)\n",
    "    print (match.group(2))  ## 'google.com' (the host, group 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common workflow with regular expressions is that you write a pattern for the thing you are looking for, adding parenthesis groups to extract the parts you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## findall\n",
    "- findall() is probably the single most powerful function in the re module. Above we used re.search() to find the first match for a pattern. findall() finds *all* the matches and returns them as a list of strings, with each string representing one match.\n",
    "- So it will basically work like a search but it will find all the matching expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice@google.com\n",
      "bob@abc.com\n"
     ]
    }
   ],
   "source": [
    "## Suppose we have a text with many email addresses\n",
    "str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "\n",
    "## Here re.findall() returns a list of all the found email strings\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', str) ## ['alice@google.com', 'bob@abc.com']\n",
    "\n",
    "for email in emails:\n",
    "    print (email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## findall With Files\n",
    "- Feed the whole file text into findall() and it will return a list of all the matches in a single step (recall that f.read() returns the whole text of a file in a single string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "f = open('test.txt', 'r')\n",
    "# Feed the file text into findall(); it returns a list of all the found strings\n",
    "strings = re.findall(r'some pattern', f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## findall and Groups\n",
    "\n",
    "- The parenthesis ( ) group mechanism can be combined with findall(). If the pattern includes 2 or more parenthesis groups, then instead of returning a list of strings, findall() returns a list of *tuples*. Each tuple represents one match of the pattern, and inside the tuple is the group(1), group(2) .. data. So if 2 parenthesis groups are added to the email pattern, then findall() returns a list of tuples, each length 2 containing the username and host, e.g. ('alice', 'google.com')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alice', 'google.com'), ('bob', 'abc.com')]\n",
      "alice\n",
      "google.com\n",
      "bob\n",
      "abc.com\n"
     ]
    }
   ],
   "source": [
    "  str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "  tuples = re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str)\n",
    "  print (tuples)  ## [('alice', 'google.com'), ('bob', 'abc.com')]\n",
    "  for tuple in tuples:\n",
    "    print (tuple[0])  ## username\n",
    "    print (tuple[1])  ## host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub(pattern, repl, string, count=0, flags=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the substitute function. It returns the string obtained by replacing or substituting the leftmost non-overlapping occurrences of pattern in string by the replacement repl. **If the pattern is not found then the string is returned unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please contact us at: support@datacamp.com\n"
     ]
    }
   ],
   "source": [
    "email_address = \"Please contact us at: xyz@datacamp.com\"\n",
    "new_email_address = re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)', r'support@datacamp.com', email_address)\n",
    "print(new_email_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile(pattern, flags=0)\n",
    "Compiles a regular expression pattern into a regular expression object. When you need to use an expression several times in a single program, using the compile() function to save the resulting regular expression object for reuse is more efficient. This is because the compiled versions of the most recent patterns passed to compile() and the module-level matching functions are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookie'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r\"cookie\")\n",
    "sequence = \"Cake and cookie\"\n",
    "pattern.search(sequence).group()\n",
    "\n",
    "# This is equivalent to:\n",
    "#re.search(pattern, sequence).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "the_idiot_url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n",
    "\n",
    "def get_book(url):\n",
    "    # Sends a http request to get the text from project Gutenberg\n",
    "    raw = requests.get(url).text\n",
    "    # Discards the metadata from the beginning of the book\n",
    "    start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    "    # Discards the metadata from the end of the book\n",
    "    stop = re.search(r\"II\", raw).start()\n",
    "    # Keeps the relevant text\n",
    "    text = raw[start:stop]\n",
    "    return text\n",
    "\n",
    "def preprocess(sentence): \n",
    "    return re.sub('[^A-Za-z0-9.]+' , ' ', sentence).lower()\n",
    "\n",
    "book = get_book(the_idiot_url)\n",
    "processed_book = preprocess(book)\n",
    "#print(processed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the number of the pronoun \"the\" in the corpus\n",
    "len(re.findall(r'the', processed_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert every single stand-alone instance of 'i' to 'I' in the corpus. Make sure not to change the 'i' occuring in a word:\n",
    "#using space hence did not convert the i near the period\n",
    "processed_book = re.sub(r'\\si\\s', \" I \", processed_book)\n",
    "#print(processed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironical--it',\n",
       " 'malicious--smile',\n",
       " 'fur--or',\n",
       " 'astrachan--overcoat',\n",
       " 'it--the',\n",
       " 'Italy--was',\n",
       " 'malady--a',\n",
       " 'money--and',\n",
       " 'little--to',\n",
       " 'No--Mr',\n",
       " 'is--where',\n",
       " 'I--I',\n",
       " 'I--',\n",
       " '--though',\n",
       " 'crime--we',\n",
       " 'or--judge',\n",
       " 'gaiters--still',\n",
       " '--if',\n",
       " 'through--well',\n",
       " 'say--through',\n",
       " 'however--and',\n",
       " 'Epanchin--oh',\n",
       " 'too--at',\n",
       " 'was--and',\n",
       " 'Andreevitch--that',\n",
       " 'everyone--that',\n",
       " 'reduce--or',\n",
       " 'raise--to',\n",
       " 'listen--and',\n",
       " 'history--but',\n",
       " 'individual--one',\n",
       " 'yes--I',\n",
       " 'but--',\n",
       " 't--not',\n",
       " 'me--then',\n",
       " 'perhaps--',\n",
       " 'Yes--those',\n",
       " 'me--is',\n",
       " 'servility--if',\n",
       " 'Rogojin--hereditary',\n",
       " 'citizen--who',\n",
       " 'least--goodness',\n",
       " 'memory--but',\n",
       " 'latter--since',\n",
       " 'Rogojin--hung',\n",
       " 'him--I',\n",
       " 'anything--she',\n",
       " 'old--and',\n",
       " 'you--scarecrow',\n",
       " 'certainly--certainly',\n",
       " 'father--I',\n",
       " 'Barashkoff--I',\n",
       " 'see--and',\n",
       " 'everything--Lebedeff',\n",
       " 'about--he',\n",
       " 'now--I',\n",
       " 'Lihachof--',\n",
       " 'Zaleshoff--looking',\n",
       " 'old--fifty',\n",
       " 'so--and',\n",
       " 'this--do',\n",
       " 'day--not',\n",
       " 'that--',\n",
       " 'do--by',\n",
       " 'know--my',\n",
       " 'illness--I',\n",
       " 'well--here',\n",
       " 'fellow--you']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What are the words connected by '--' in the corpus?\n",
    "\n",
    "re.findall(r'[a-zA-Z0-9]*--[a-zA-Z0-9]*', book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline and Column Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used for Applying different transformations to different features in a scikit-learn pipeline\n",
    "- Available from release 0.20 onwards\n",
    "\n",
    "- Real-world data often contains heterogeneous data types. When processing the data before applying the final prediction model, we typically want to use different preprocessing steps and transformations for those different types of columns.\n",
    "\n",
    "- A simple example: we may want to scale the numerical features and one-hot encode the categorical features.\n",
    "- Putting preprocessing steps in a scikit-learn Pipeline is important to avoid data leakage or to do a grid search over preprocessing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"https://raw.githubusercontent.com/amueller/scipy-2017-sklearn/master/notebooks/datasets/titanic3.csv\")\n",
    "#print (titanic.head().T)\n",
    "#dropping the missing values for simplcity\n",
    "\n",
    "titanic2 = titanic.dropna(subset=['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])\n",
    "titanic2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex      age      fare embarked\n",
       "0       1  female  29.0000  211.3375        S\n",
       "1       1    male   0.9167  151.5500        S\n",
       "2       1  female   2.0000  151.5500        S\n",
       "3       1    male  30.0000  151.5500        S\n",
       "4       1  female  25.0000  151.5500        S"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = titanic2.survived.values\n",
    "features = titanic2[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains some categorical variables (\"pclass\", \"sex\" and \"embarked\"), and some numerical variables (\"age\" and \"fare\"). Note that the \"pclass\", although categorical, is already encoded as integers in the dataset. So let's use the ColumnTransformer to combine transformers for those two types of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:732: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (['age', 'fare'], StandardScaler()),\n",
    "    (['pclass', 'sex', 'embarked'], OneHotEncoder())\n",
    ")\n",
    "#create a pipeline for numerical features and categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above creates a simple preprocessing pipeline (that will be combined in a full prediction pipeline below) to scale the numerical features and one-hot encode the categorical features.\n",
    "We can check this is indeed working as expected by transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05663194,  3.13554913,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-2.01237899,  2.06268333,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-1.93693697,  2.06268333,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [ 0.01300899,  2.06268333,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.33519565,  2.06268333,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check that the input data has been transformed, print the transformed verison of array\n",
    "preprocess.fit_transform(features)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating in a full pipeline\n",
    "Example to integrate the ColumnTransformer in a prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the full dataset this time including the missing values:\n",
    "\n",
    "target = titanic.survived.values\n",
    "features = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']].copy()\n",
    "\n",
    "# Filling missing values in embarked to simply handle categorical missing values\n",
    "\n",
    "features['embarked'].fillna(features['embarked'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's split the data in training and testing data:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting numerical and categorical columns based on their data types\n",
    "numerical_features = features.dtypes == 'float'\n",
    "categorical_features = ~numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:732: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#pipeline inside pipeline\n",
    "\n",
    "#for numerical data, make another pipeline for imputation and scaling and then put it into this pipeline\n",
    "#for categorical data, directly apply one hot encoder\n",
    "\n",
    "#simple imputer will do imputation of missing values\n",
    "#standard scalar will do scaling and standardization of feature values\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (numerical_features, make_pipeline(SimpleImputer(), StandardScaler())),\n",
    "    (categorical_features, OneHotEncoder()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine this preprocessing step based on the ColumnTransformer with a classifier in a Pipeline to predict whether passengers of the Titanic survived or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.786585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(\n",
    "    preprocess,\n",
    "    LogisticRegression())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"logistic regression score: %f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our pipeline in a grid searchÂ¶\n",
    "\n",
    "- Using grid search to find the best regularization parameter of the logistic regression and the imputer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#these are the parameters that I will be searching over\n",
    "param_grid = {\n",
    "    'columntransformer__pipeline__simpleimputer__strategy': ['mean', 'median'],\n",
    "    'logisticregression__C': [0.1, 1.0, 1.0],\n",
    "    }\n",
    "\n",
    "#Performing the grid search:\n",
    "\n",
    "grid_clf = GridSearchCV(model, param_grid, cv=10, iid=False)\n",
    "grid_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer__pipeline__simpleimputer__strategy': 'mean',\n",
       " 'logisticregression__C': 0.1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.792683\n"
     ]
    }
   ],
   "source": [
    "print(\"best logistic regression from grid search: %f\" % grid_clf.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Column transformer and One Hot Encoder\n",
    "The purpose of this example is to predict wether or not a loan application will be successful based on a number of customer features. This contains both categorical and numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 13) (367, 12)\n",
      "Loan_ID               object\n",
      "Gender                object\n",
      "Married               object\n",
      "Dependents            object\n",
      "Education             object\n",
      "Self_Employed         object\n",
      "ApplicantIncome        int64\n",
      "CoapplicantIncome    float64\n",
      "LoanAmount           float64\n",
      "Loan_Amount_Term     float64\n",
      "Credit_History       float64\n",
      "Property_Area         object\n",
      "Loan_Status           object\n",
      "dtype: object\n",
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.shape, test.shape)\n",
    "print(train.dtypes)\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop loan id as it does not provide any useful information for creating features\n",
    "train = train.drop(['Loan_ID'], axis=1)\n",
    "test =  test.drop(['Loan_ID'], axis=1)\n",
    "\n",
    "#Filling the missing values with the most commonly occuring ones\n",
    "\n",
    "train = train.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "test = test.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = train.drop(['Loan_Status'], axis=1)\n",
    "\n",
    "X = feature_set.columns[:len(feature_set.columns)]\n",
    "y = 'Loan_Status'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train[X], train[y], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnTransformer\n",
    "- Apply transformation to columns to optimise them for use in the classification model\n",
    "- Normalize the numerical columns using the Normalizer function.\n",
    "- The ColumnTransformer takes a list of tuples specifying the transformers, and the corresponding columns on which the transformation needs to be applied. The columns can either be entered as strings specifying the column names in a pandas data frame, or as in the code as used below, as integers which are interpreted as the column positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "\n",
    "colT = ColumnTransformer(\n",
    "    [(\"dummy_col\", OneHotEncoder(categories=[['Male', 'Female'],\n",
    "                                           ['Yes', 'No'],\n",
    "                                            ['0','1', '2','3+'],\n",
    "                                            ['Graduate', 'Not Graduate'],\n",
    "                                            ['No', 'Yes'],\n",
    "                                            ['Semiurban', 'Urban', 'Rural']]), [0,1,2,3,4,10]),\n",
    "      (\"norm\", Normalizer(norm='l1'), [5,6,7,8,9])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categories argument of the OnHotEncoder function. This takes a list of all possible categories in each column as a list of lists. This produces one hot encoded columns for all categories even if data does not exist for that category in the column. The reason for doing this is that when using the ColumnTransformer function on new data. If it doesnât contain the same categories in each feature then the array produced will not be the same shape as the data used to train the model, and you will get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example toxic commets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/train.csv')\n",
    "print (df.head())\n",
    "x = df['comment_text'].values[:5000]\n",
    "y = df['toxic'].values[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default params that we will be using with different transformations later\n",
    "scoring='roc_auc'\n",
    "cv=3\n",
    "n_jobs=-1\n",
    "max_features = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91248134, 0.92889109, 0.92437274])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple pipelines of default sklearn TfidfVectorizer to prepare features and Logistic Reegression to make predictions.\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "lr = LogisticRegression()\n",
    "p = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('lr', lr)\n",
    "])\n",
    "\n",
    "cross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create or own Estimator. This estimator is created with sklearn BaseEstimator class and needs to have fit and transform methods. First Pipeline calls fit methods to learn your dataset and then calls transform to apply knowledge and does some transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBFeaturer(BaseEstimator):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def preprocess_x(self, x, r):\n",
    "        return x.multiply(r)\n",
    "    \n",
    "    def pr(self, x, y_i, y):\n",
    "        p = x[y==y_i].sum(0)\n",
    "        return (p+self.alpha) / ((y==y_i).sum()+self.alpha)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self._r = sparse.csr_matrix(np.log(self.pr(x,1,y) / self.pr(x,0,y)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x_nb = self.preprocess_x(x, self._r)\n",
    "        return x_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91851711, 0.93572898, 0.91808511])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "lr = LogisticRegression()\n",
    "nb = NBFeaturer(1)\n",
    "p = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('nb', nb),\n",
    "    ('lr', lr)\n",
    "])\n",
    "\n",
    "cross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "2. http://queirozf.com/entries/scikit-learn-pipeline-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
