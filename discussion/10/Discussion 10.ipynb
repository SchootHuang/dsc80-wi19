{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DSC 80 Review Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "The goals of this course are to be able to understand real world data, clean them and extract meaningful insight from them.\n",
    "### Real world data types\n",
    "* tabular data (csv file)\n",
    "* unstructued text (create your own parser)\n",
    "* json (geojson)\n",
    "* web data (scrape) \n",
    "* time series data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tools\n",
    "* numpy: math computation library on array and matrices , serves as underlying data structure for pandas\n",
    "* pandas: tabular data manipulation\n",
    "* matplotlib and folium: visualization \n",
    "* regex: pattern matching on text\n",
    "* requests and BeautifulSoup: scraping tools\n",
    "* scikit: modeling and analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data science lifecycle\n",
    "* Ask questions, define metrics, form hypotheses\n",
    "* Find data (if it doesn't already exist)\n",
    "* Clean data into organized format for analysis\n",
    "* Find anomalies, biases and simplify data (imputation, smoothing, etc.)\n",
    "* Model data (build classifier, conduct A/B experiment or hypothesis testing)\n",
    "* Evaluate rusults (and reiterate!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas\n",
    "* Know the difference between DataFrame and Series, indexing, and the function associate with them\n",
    "* Know which column should be which data type, how to convert between data types and properties of each dtype\n",
    "    - object\n",
    "    - int\n",
    "    - datetime\n",
    "    - float\n",
    "* Group by and aggregate function\n",
    "* Filter rows with conditions and merge (left_on, right_on, etc.)\n",
    "* Write lambda functions (df.apply())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Cleaning\n",
    "* Data types: quantitative, ordinal, nominal\n",
    "* Unfaithful data (data that doesn't represent the data generating process being measured)\n",
    "    - solution -> case specific but we can drop them or replace them\n",
    "* Outliers data (extreme values) \n",
    "    - we can correct them if we know they're wrong for sure, or try smoothing them out or filter out some unreasonable ones\n",
    "* Handling Nans -> fillna() or dropna() depending on the situation\n",
    "* Understand the cause of missingness -> MD, MCAR, MAR or NMAR\n",
    "* Having domain knowledge will allow you to make decisions on how to concat data, merge, join them on row or on column on which key (e.g. College Scorecard dataset example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothing\n",
    "* Extract trend from data -> separate signal from noise\n",
    "* Distribution -> likelihood that a given value x will occur. Can be quantitative or categorical\n",
    "* Empirical distribution, how frequent each value occurs\n",
    "    - Represented as histogram\n",
    "    - To plot histogram we can put data into bins or percentiles\n",
    "* Smoothing reduces the extremeness in the outlier\n",
    "* Noise causes more error in events with rare occurence -> motivation for additive smoothing\n",
    "* Additive smoothing -> define alpha as how much we uncertain about the data collected. \n",
    "    - $p_i = \\frac{x_i + \\alpha}{N + \\alpha d}$ \n",
    "    - Big alpha will converge to uniform distribution, small alpha will converge to empirical distribution\n",
    "* Rolling windows -> apply a function on a rolling basis of the data, generally used for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Total Variation Distance\n",
    "* Measure how similar two distributions are -> help us determine the type of missingness\n",
    "* The sum of the absolute difference between the terms of $X$  and the terms in  $Y$ , divided by two\n",
    "    - $TVD = \\Sigma_i \\frac{|x_i - y_i|}{2}$\n",
    "\n",
    "* Absolute because we want to consider only the magnitude, and we divide by 2 to account for that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imputation\n",
    "* Imputation with single values e.g. mean or median, will reduce the variance\n",
    "    - Mean imputation -> preserve the mean of the observed data e.g. when we know the missing value is MCAR\n",
    "    - Group-wise mean imputation -> preserve mean within one group e.g. when encounter MAR\n",
    "* Sampling from distribution imputation -> use empirical distribution to sample values to fillna() in MCAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON\n",
    "* key-value data structure -> dictionary like\n",
    "* GeoJson\n",
    "* Request/Response format\n",
    "\n",
    "## HTTPs\n",
    "* HTTP request/response consist of header and body. Header has metadata and body has the content\n",
    "* Difference between get and post requests\n",
    "    - GET retrieves data, params parsed with URL, restricted length and no sensitive info\n",
    "    - POST sends data, params embeded in the request body, unlimited length and can contain any data format\n",
    "* HTTP status code tells us the result of the request e.g. 200 means ok\n",
    "* To send a request, determine which API endpoints to call and what params are required\n",
    "* Sending too many get requests may get you blocked \n",
    "    - Check robots.txt file regarding the scraping policy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing HTML\n",
    "* HTML consists of DOM tags\n",
    "* Parse using BeautifulSoup \n",
    "* Traverse the objects to extract the information\n",
    "    - Tree traversal e.g. DFS, BFS\n",
    "    - Calling parent(), children(), next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning on Unstructured Data \n",
    "\n",
    "- We've seen how to extract and collect data from different sources\n",
    "- This data is in unstructured form \n",
    "- we need to structurize it to be able to work with it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonicalization\n",
    "\n",
    "* a sequence of steps that transforms both columns into a single form\n",
    "- Replace each string with a unique representation\n",
    "- in the following image, we try to transform the 2 columns of `county`\n",
    "\n",
    "<img src=\"image_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Cons:\n",
    "- Used string methods\n",
    "- Very brittle procedure; may only work for X% of the data.\n",
    "- Hard to verify correctness.\n",
    "- Also *parse* data using a data model if given the choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular Expressions (`regexp`)\n",
    "\n",
    "* Fast, compact way of matching patterns in text\n",
    "* Python library: `import re`\n",
    "\n",
    "\n",
    "* Pros: powerful; capable of matching very complex patterns.\n",
    "* Cons: \n",
    "    - It's still text processing, so brittle and likely to break.\n",
    "    - Hard to understand: \"write-only\" language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regexp Expressions\n",
    "* Parsing the expression:\n",
    "```\n",
    "'\\[([0-9]{2}\\/[A-Z]{1}[a-z]{2}\\/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2} -[0-9]{4})\\]'\n",
    "```\n",
    "\n",
    "* `[0-9]{2}` matches any 2-digit number.\n",
    "* `[A-Z]{1}` matches any single occurrence of any upper-case letter.\n",
    "* `[a-z]{2}` matches any 2 consecutive occurrences of lower-case letters.\n",
    "* Certain special characters (`[`, `]`, `/`) need to be escaped with `\\`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Patterns\n",
    "The power of regular expressions is that they can specify patterns, not just fixed characters. Here are the most basic patterns which match single chars:\n",
    "\n",
    "- **a, X, 9, <** -- ordinary characters just match themselves exactly\n",
    "- The meta-characters which do not match themselves because they have special meanings are: **. ^ $ * + ? { [ ] \\ | ( )** (details below)\n",
    "\n",
    "-  **. (a period)** -- matches any single character except newline '\\n'\n",
    "- **\\w** -- (lowercase w) matches a \"word\" character: a letter or digit or underbar [a-zA-Z0-9_]. **Note that although \"word\" is the mnemonic for this, it only matches a single word char, not a whole word**. \\W (upper case W) matches any non-word character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **\\s** -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form [ \\n\\r\\t\\f]. \\S (upper case S) matches any non-whitespace character.\n",
    "\n",
    "- **\\t, \\n, \\r** -- tab, newline, return\n",
    "\n",
    "- **\\d** -- decimal digit [0-9] (some older regex utilities do not support but \\d, but they all support \\w and \\s)\n",
    "\n",
    "- **^ = start, $ = end** -- match the start or end of the string\n",
    "\n",
    "- **\\** -- inhibit the \"specialness\" of a character. So, for example, use \\. to match a period or \\\\ to match a slash. If you are unsure if a character has special meaning, such as '@', you can put a slash in front of it, \\@, to make sure it is treated just as a character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python `re` library functions\n",
    "* `re.search`:\n",
    "    - `m = re.search(r, s); m.groups`\n",
    "* `re.findall`\n",
    "* `re.sub`\n",
    "\n",
    "Also in Pandas!\n",
    "\n",
    "### Regexp Groups (Briefly)\n",
    "* Use `(` regex `)` to *extract* text from a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bag of Words\n",
    "\n",
    "* Create an index out of *all* distinct words \n",
    "    - The basis for the vector space of words.\n",
    "* Create vectors for each text entry by computing the counts of words in the entry.\n",
    "* The dot product between two vectors is proportional to their 'similarity':\n",
    "    - This defines the **cosine similarity** between vectors via: $$dist(v, w) = 1 - \\cos(\\theta) = 1 - \\frac{v \\cdot w}{|v||w|}$$\n",
    "\n",
    "\n",
    "* Downside of `bag of words`: treats all words as *equally important*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency, Inverse Document Frequency\n",
    "\n",
    "* The *term frequency* of a word $t$ in a document $d$, denoted ${\\rm tf}(t,d)$, is the likelihood of the term appearing in the document.\n",
    "* The *inverse document frequency* of a word $t$ in a set of documents $\\{d_i\\}$, denoted ${\\rm idf}(t,d)$ is: \n",
    "\n",
    "$$\\log(\\frac{{\\rm\\ total\\ number\\ of\\ documents}}{{\\rm number\\ of\\ documents\\ in\\ which\\ t\\ appears}})$$\n",
    "\n",
    "* The *tf-idf* of a term $t$ in document $d$ is given by the product: \n",
    "\n",
    "$${\\rm tfidf}(t,d) = {\\rm tf}(t,d) \\cdot {\\rm idf}(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Term Frequency, Inverse Document Frequency balances:\n",
    "    - how often a word appears in a document/sentence, with\n",
    "    - how often a word appears *across* documents.\n",
    "* For a given document, the word with the highest TF-IDF best summarizes that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a feature?\n",
    "\n",
    "* A **feature** is a measurable property or characteristic of a phenomenon being observed.\n",
    "* Synonyms: (explanatory) variable, attribute\n",
    "* Examples include:\n",
    "    - a column of a dataset.\n",
    "    - a derived value from a dataset, perhaps using additional information.\n",
    "    \n",
    "We have been creating features to summarize data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "* We already engineered features to summarize and understand data.\n",
    "    - smoothing, transformations, ad hoc derived properties of data\n",
    "\n",
    "* What can we do with it?\n",
    "    - Visualization and summarization\n",
    "    - Modeling (prediction; inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling setup\n",
    "\n",
    "Want to estimate a relationship between X and Y.\n",
    "* X is the observed data (almost anything!)\n",
    "* Y is a quantitative value (e.g. a correlation coefficient; a predicted value)\n",
    "\n",
    "<img src=\"image_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The missing step: data to models\n",
    "\n",
    "* Modeling techniques typically require *quantitative* input.\n",
    "* Models require (strong) relationships between X and Y.\n",
    "\n",
    "<img src=\"image_3.png\">\n",
    "\n",
    "There is work to be done transforming data into effective features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The goal of feature engineering\n",
    "\n",
    "* Find transformations that effectively transform data into effective quantitative variables\n",
    "\n",
    "* Find functions $\\phi:X\\to\\mathbb{R}^d$ where similar points $x,y\\in X$ have close images $\\phi(x), \\phi(y)\\in \\mathbb{R}^d$\n",
    "\n",
    "* A \"good\" choice of features depends on many factors:\n",
    "    - data type (quantitative, ordinal, nominal),\n",
    "    - the relationship(s) and association(s) being modeled,\n",
    "    - the model type (e.g. linear models, decision tree models, neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The modeling pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The steps of the modeling pipeline\n",
    "\n",
    "1. We have already seen Feature Engineering, ie. to create features to best reflect the meaning behind data\n",
    "2. Create model appropriate to capture relationships between features\n",
    "    - e.g. linear, non-linear\n",
    "3. Select a loss function and fit the model (determine $\\hat{\\theta}$).\n",
    "4. Evaluate model (e.g. using RMSE)\n",
    "\n",
    "After these steps, use the model for prediction and/or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " The pipeline above represents a single attempt at a model\n",
    "    - May have thousands of feature/model/paramater combinations to choose from!\n",
    "    - Remember the Data Science Life Cycle!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features and Models using `Scikit Learn`\n",
    "\n",
    "* Scikit-Learn implements many common steps in the feature/model creation pipeline.\n",
    "* It interfaces with `numpy` arrays, *not* Pandas dataframes :(\n",
    "    - Some work required keeping track of columns in scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of a Scikit-Learn Model \n",
    "\n",
    "* Initialize a model with (perhaps zero) parameters:\n",
    "    - e.g. `lr = LinearRegression()`\n",
    "* Fit model to given dataset using `.fit`\n",
    "    - e.g. `lr.fit(data, outcomes)` fits the model weights using `data` and `outcomes`.\n",
    "* Use the model to predict using `.predict` method\n",
    "    - e.g. `lr.predict(newdata)` predicts outcomes for `newdata`.\n",
    "* Inspect model attributes, like model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model\n",
    "\n",
    "* Given a fit model on dataset, calculate e.g. the root-mean-square error.\n",
    "* If the error is low, do you think it's a good model?\n",
    "    - It fits the given *data* well, but is it a good model? (Is the sample representative?)\n",
    "    - E.g. will it give good predictions on similar, unknown, data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fundamental Concepts of the quality of a 'fit model'\n",
    "\n",
    "* **Bias**: the expected deviation between the predicted value and true value\n",
    "* **Variance**:\n",
    "    - **Observation Variance**: the variability of the random noise in the process we are trying to model. \n",
    "    - **Estimated Model Variance**: the variability in the predicted value across different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A more detailed understanding of 'model quality'\n",
    "\n",
    "* Accuracy is defined as the proportion of predictions that are correct.\n",
    "    - treats all incorrect guesses equally\n",
    "    - treats all correct guesses equally\n",
    "* Ignores *how* the predictions were (in)correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary Classification Outcomes\n",
    "\n",
    "* **True positive (TP)**: the model correctly predicts the positive class.\n",
    "\n",
    "* **True negative (TN)**: the model correctly predicts the negative class.\n",
    "\n",
    "* **False positive (FP)**:the model incorrectly predicts the positive class. \n",
    "\n",
    "* **False negative (FN)**: the model incorrectly predicts the negative class. \n",
    "\n",
    "\n",
    "\n",
    "In order to estimate the accuracy of the classifier, we need to know the number of real positive cases in the data **P**, ie. TP+FN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1-score: combining precision and recall: \n",
    "\n",
    "\n",
    "* **F1-score** combines precision and recall via the 'harmonic mean'.\n",
    "\n",
    "* The F1 score is a measure of how well a test labels positive instances. \n",
    "\n",
    "* It considers both the precision and the recall (TPR) of the test to compute the score.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{Precision * Recall}{Precision + Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### False Discovery Rate (FDR)\n",
    "\n",
    "* The proportion of positive identifications that were false (positives).\n",
    "* Terrorism Example: the proportion of people flagged as terrorists who are normal passengers.\n",
    "    - A high FDR leads to a lot of average people inconvenienced.\n",
    "* Related to precision (FDR = 1 - Precision).\n",
    "\n",
    "$$\n",
    "{\\rm FDR} =\\frac{FP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sensitivity, Specificity, Precision and Recall\n",
    "\n",
    "Sensitivity: \n",
    "- What proportion of actual positives were correctly identified?\n",
    "* Also called: True positive rate (TPR), hit-rate, recall.\n",
    "\n",
    "$${\\rm Sensitivity/ Recall} = {\\rm TPR} = \\frac{TP}{P} =\\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "Specificity\n",
    "- What proportion of actual negatives were correctly identified?\n",
    "* Also called: Selectivity, true negative rate (TNR).\n",
    "\n",
    "$$\n",
    "{\\rm Specificity} =\\frac{TN}{N} = \\frac{TN}{TN + FP} \n",
    "$$\n",
    "\n",
    "Precision:\n",
    "- What proportion of positive identifications were actually correct?\n",
    "\n",
    "$$\n",
    "{\\rm Precision} =\\frac{TP}{TP + FP}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-Test Split\n",
    "\n",
    "To assess your model for overfitting to the data, randomly split the data into a \"training set\" and a \"test set\".\n",
    "\n",
    "- The training set is used to fit the model (train the predictor).\n",
    "-  The test set is used to test the goodness-of-fit of the fit model.\n",
    "- *similar* to bootstrap estimating a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The machine learning training pipeline:\n",
    "\n",
    "<img src=\"train-test.png\" width=\"50%\">\n",
    "\n",
    "Scikit-Learn as functions that help us do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to put together Scikit-Learn Pipelines\n",
    "- Put together feature transformers and models using sklearn.Pipeline objects\n",
    "- Create a pipeline: <i>pl = Pipeline([feat, mdl])</i>\n",
    "- Fit the model(s) in the pipeline using pl.fit(data, target)\n",
    "- Predict from raw input data through the pipeline using pl.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Example of a Pipeline\n",
    "\n",
    "- We use the iris dataset\n",
    "- We perform pre-preprocessing by standardizing the data\n",
    "- We use a Logistic Regressor to classify the dataset into its target iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size= 0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('stdscr', StandardScaler()),\n",
    " ('clf', LogisticRegression(solver='newton-cg', multi_class='ovr'))])\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('stdscr', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "score = pipe_lr.score(X_test, y_test)\n",
    "print('Logistic Regression pipeline test accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Function Transformer\n",
    "- Recall what a function transformer is\n",
    "- It forwards the X (and optionally y) arguments to a user-defined function or function object and returns the result of this function\n",
    "- Used in Data Pre-processing\n",
    "- Somewhat like an `apply` in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But what if we cannot apply the same transformations to every individual feature of a data point in X?\n",
    "This is why we need `Column Transformers`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Column Transformer\n",
    "- Datasets can often contain components that require different feature extraction and processing pipelines\n",
    "- Datasets may have a mix of Categorical columns and Continuous Numeric columns, which will almost always need separate transformations\n",
    "- Datasets may be stored in a Pandas DataFrame and different columns require different processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Example:\n",
    "    - Your dataset consists of heterogeneous data types (e.g. raster images and text captions)\n",
    "    - You want to standardize the numerical columns but one-hot-encode the categorical ones\n",
    "\n",
    "- The brand new ColumnTransformer allows you to choose which columns get which transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The ColumnTransformer takes a list of tuples, where each tuple has the following 3 entries:\n",
    "    - The first value in the tuple is a name that labels it, \n",
    "    - the second is an instantiated estimator or transformation, \n",
    "    - and the third is a list of columns you want to apply the transformation to. \n",
    "- The tuple will look like this:\n",
    "     `('name', SomeTransformer(parameters), columns)`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
